package com.linlay.springaiagw.service;

import com.fasterxml.jackson.databind.ObjectMapper;
import com.linlay.springaiagw.config.AgentProviderProperties;
import com.linlay.springaiagw.config.ChatClientRegistry;
import com.linlay.springaiagw.config.LlmInteractionLogProperties;
import com.linlay.springaiagw.model.ProviderType;
import com.linlay.springaiagw.model.SseChunk;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.ai.chat.client.ChatClient;
import org.springframework.ai.chat.messages.AssistantMessage;
import org.springframework.ai.chat.messages.Message;
import org.springframework.ai.chat.model.ChatResponse;
import org.springframework.ai.openai.OpenAiChatOptions;
import org.springframework.ai.openai.api.OpenAiApi;
import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.stereotype.Service;
import org.springframework.util.StringUtils;
import reactor.core.publisher.Flux;
import reactor.core.publisher.Mono;
import reactor.core.scheduler.Schedulers;

import java.time.Duration;
import java.util.LinkedHashMap;
import java.util.List;
import java.util.Locale;
import java.util.Map;
import java.util.concurrent.atomic.AtomicBoolean;

@Service
public class LlmService {

    private static final Logger log = LoggerFactory.getLogger(LlmService.class);

    private final ChatClientRegistry chatClientRegistry;
    private final RawSseClient rawSseClient;
    private final LlmCallLogger callLogger;
    private final Map<String, ChatClient> legacyClients;

    public record LlmFunctionTool(
            String name,
            String description,
            Map<String, Object> parameters,
            Boolean strict
    ) {
    }

    public record LlmStreamDelta(
            String content,
            List<SseChunk.ToolCall> toolCalls,
            String finishReason
    ) {
    }

    public LlmService(ChatClientRegistry chatClientRegistry) {
        this(
                chatClientRegistry,
                new AgentProviderProperties(),
                new ObjectMapper(),
                new LlmInteractionLogProperties(),
                Map.of()
        );
    }

    @Autowired
    public LlmService(
            ChatClientRegistry chatClientRegistry,
            AgentProviderProperties providerProperties,
            ObjectMapper objectMapper,
            LlmInteractionLogProperties logProperties
    ) {
        this(chatClientRegistry, providerProperties, objectMapper, logProperties, Map.of());
    }

    /**
     * Backward-compatible constructor for tests that historically injected two ChatClient instances.
     */
    @Deprecated
    public LlmService(ChatClient bailianChatClient, ChatClient siliconflowChatClient) {
        this(
                null,
                new AgentProviderProperties(),
                new ObjectMapper(),
                new LlmInteractionLogProperties(),
                legacyClientMap(bailianChatClient, siliconflowChatClient)
        );
    }

    private LlmService(
            ChatClientRegistry chatClientRegistry,
            AgentProviderProperties providerProperties,
            ObjectMapper objectMapper,
            LlmInteractionLogProperties logProperties,
            Map<String, ChatClient> legacyClients
    ) {
        this.chatClientRegistry = chatClientRegistry;
        this.callLogger = new LlmCallLogger(logProperties);
        this.rawSseClient = new RawSseClient(providerProperties, objectMapper, this.callLogger);
        this.legacyClients = legacyClients == null ? Map.of() : Map.copyOf(legacyClients);
    }

    public Flux<String> streamContent(String providerKey, String model, String userPrompt) {
        return streamContent(providerKey, model, null, userPrompt, "default");
    }

    public Flux<String> streamContent(String providerKey, String model, String systemPrompt, String userPrompt) {
        return streamContent(providerKey, model, systemPrompt, userPrompt, "default");
    }

    public Flux<String> streamContent(
            String providerKey,
            String model,
            String systemPrompt,
            String userPrompt,
            String stage
    ) {
        ProviderType providerType = toProviderType(providerKey);
        if (providerType != null) {
            return streamContent(providerType, model, systemPrompt, userPrompt, stage);
        }
        return streamContentInternal(providerKey, model, systemPrompt, List.of(), userPrompt, stage);
    }

    public Flux<String> streamContent(
            String providerKey,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            String stage
    ) {
        ProviderType providerType = toProviderType(providerKey);
        if (providerType != null) {
            return streamContent(providerType, model, systemPrompt, historyMessages, userPrompt, stage);
        }
        return streamContentInternal(providerKey, model, systemPrompt, historyMessages, userPrompt, stage);
    }

    @Deprecated
    public Flux<String> streamContent(ProviderType providerType, String model, String userPrompt) {
        return streamContent(providerType, model, null, userPrompt, "default");
    }

    @Deprecated
    public Flux<String> streamContent(ProviderType providerType, String model, String systemPrompt, String userPrompt) {
        return streamContent(providerType, model, systemPrompt, userPrompt, "default");
    }

    @Deprecated
    public Flux<String> streamContent(
            ProviderType providerType,
            String model,
            String systemPrompt,
            String userPrompt,
            String stage
    ) {
        return streamContentInternal(resolveProviderKey(providerType), model, systemPrompt, List.of(), userPrompt, stage);
    }

    @Deprecated
    public Flux<String> streamContent(
            ProviderType providerType,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            String stage
    ) {
        String providerKey = resolveProviderKey(providerType);
        ChatClient chatClient = resolveChatClient(providerKey);
        if (chatClient == null) {
            // Compatibility bridge for tests/legacy callers overriding ProviderType streamContent.
            return streamContent(providerType, model, systemPrompt, userPrompt, stage);
        }
        return streamContentInternal(providerKey, model, systemPrompt, historyMessages, userPrompt, stage);
    }

    public Flux<LlmStreamDelta> streamDeltas(
            String providerKey,
            String model,
            String systemPrompt,
            String userPrompt,
            String stage
    ) {
        return streamDeltas(providerKey, model, systemPrompt, List.of(), userPrompt, List.of(), stage);
    }

    public Flux<LlmStreamDelta> streamDeltas(
            String providerKey,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            List<LlmFunctionTool> tools,
            String stage
    ) {
        return streamDeltas(providerKey, model, systemPrompt, historyMessages, userPrompt, tools, stage, false);
    }

    public Flux<LlmStreamDelta> streamDeltas(
            String providerKey,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            List<LlmFunctionTool> tools,
            String stage,
            boolean parallelToolCalls
    ) {
        ProviderType providerType = toProviderType(providerKey);
        if (providerType != null) {
            return streamDeltas(providerType, model, systemPrompt, historyMessages, userPrompt, tools, stage, parallelToolCalls);
        }
        return streamDeltasInternal(providerKey, model, systemPrompt, historyMessages, userPrompt, tools, stage, parallelToolCalls);
    }

    @Deprecated
    public Flux<LlmStreamDelta> streamDeltas(
            ProviderType providerType,
            String model,
            String systemPrompt,
            String userPrompt,
            String stage
    ) {
        return streamDeltas(providerType, model, systemPrompt, List.of(), userPrompt, List.of(), stage, false);
    }

    @Deprecated
    public Flux<LlmStreamDelta> streamDeltas(
            ProviderType providerType,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            List<LlmFunctionTool> tools,
            String stage
    ) {
        return streamDeltas(providerType, model, systemPrompt, historyMessages, userPrompt, tools, stage, false);
    }

    @Deprecated
    public Flux<LlmStreamDelta> streamDeltas(
            ProviderType providerType,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            List<LlmFunctionTool> tools,
            String stage,
            boolean parallelToolCalls
    ) {
        String providerKey = resolveProviderKey(providerType);
        ChatClient chatClient = resolveChatClient(providerKey);
        if (chatClient == null) {
            // Compatibility bridge for tests/legacy callers overriding ProviderType streamContent.
            return streamContent(providerType, model, systemPrompt, userPrompt, stage)
                    .map(content -> new LlmStreamDelta(content, null, null));
        }
        return streamDeltasInternal(providerKey, model, systemPrompt, historyMessages, userPrompt, tools, stage, parallelToolCalls);
    }

    public Flux<String> streamContentRawSse(
            String providerKey,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            String stage
    ) {
        return rawSseClient.streamContentRawSse(providerKey, model, systemPrompt, historyMessages, userPrompt, stage);
    }

    public Mono<String> completeText(String providerKey, String model, String systemPrompt, String userPrompt) {
        return completeText(providerKey, model, systemPrompt, userPrompt, "default");
    }

    public Mono<String> completeText(
            String providerKey,
            String model,
            String systemPrompt,
            String userPrompt,
            String stage
    ) {
        ProviderType providerType = toProviderType(providerKey);
        if (providerType != null) {
            return completeText(providerType, model, systemPrompt, userPrompt, stage);
        }
        return completeTextInternal(providerKey, model, systemPrompt, userPrompt, stage);
    }

    @Deprecated
    public Mono<String> completeText(ProviderType providerType, String model, String systemPrompt, String userPrompt) {
        return completeText(providerType, model, systemPrompt, userPrompt, "default");
    }

    @Deprecated
    public Mono<String> completeText(
            ProviderType providerType,
            String model,
            String systemPrompt,
            String userPrompt,
            String stage
    ) {
        return completeTextInternal(resolveProviderKey(providerType), model, systemPrompt, userPrompt, stage);
    }

    private Flux<String> streamContentInternal(
            String providerKey,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            String stage
    ) {
        ChatClient chatClient = resolveChatClient(providerKey);
        if (chatClient == null) {
            return Flux.error(new IllegalStateException("No ChatClient registered for provider key: " + providerKey));
        }

        return Flux.defer(() -> {
            String traceId = callLogger.generateTraceId();
            long startNanos = System.nanoTime();
            StringBuilder responseBuffer = new StringBuilder();

            callLogger.info(log, "[{}][{}] LLM stream request start provider={}, model={}", traceId, stage, providerKey, model);
            callLogger.info(log, "[{}][{}] LLM stream system prompt:\n{}", traceId, stage, callLogger.normalizePrompt(systemPrompt));
            callLogger.info(log, "[{}][{}] LLM stream history messages count={}", traceId, stage, historyMessages == null ? 0 : historyMessages.size());
            callLogger.logHistoryMessages(log, traceId, stage, historyMessages);
            callLogger.info(log, "[{}][{}] LLM stream user prompt:\n{}", traceId, stage, callLogger.normalizePrompt(userPrompt));

            OpenAiChatOptions options = OpenAiChatOptions.builder().model(model).build();
            ChatClient.ChatClientRequestSpec prompt = chatClient.prompt().options(options);
            if (StringUtils.hasText(systemPrompt)) {
                prompt = prompt.system(systemPrompt);
            }
            if (historyMessages != null && !historyMessages.isEmpty()) {
                prompt = prompt.messages(historyMessages);
            }

            return prompt.user(userPrompt)
                    .stream()
                    .content()
                    .doOnNext(chunk -> {
                        if (chunk != null) {
                            String safeChunk = callLogger.sanitizeText(chunk);
                            responseBuffer.append(safeChunk);
                            callLogger.debug(log, "[{}][{}][delta] content: {}", traceId, stage, safeChunk);
                        }
                    })
                    .doOnComplete(() -> callLogger.info(
                            log,
                            "[{}][{}] LLM stream response finished in {} ms:\n{}",
                            traceId,
                            stage,
                            callLogger.elapsedMs(startNanos),
                            responseBuffer
                    ))
                    .doOnError(ex -> log.error(
                            "[{}][{}] LLM stream failed in {} ms, partial response:\n{}",
                            traceId,
                            stage,
                            callLogger.elapsedMs(startNanos),
                            responseBuffer,
                            ex
                    ))
                    .doOnCancel(() -> callLogger.info(
                            log,
                            "[{}][{}] LLM stream canceled in {} ms, partial response:\n{}",
                            traceId,
                            stage,
                            callLogger.elapsedMs(startNanos),
                            responseBuffer
                    ))
                    .timeout(Duration.ofSeconds(60));
        });
    }

    private Flux<LlmStreamDelta> streamDeltasInternal(
            String providerKey,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            List<LlmFunctionTool> tools,
            String stage,
            boolean parallelToolCalls
    ) {
        return Flux.defer(() -> {
            String traceId = callLogger.generateTraceId();
            long startNanos = System.nanoTime();
            StringBuilder responseBuffer = new StringBuilder();
            ChatClient chatClient = resolveChatClient(providerKey);
            boolean hasTools = tools != null && !tools.isEmpty();

            callLogger.info(log, "[{}][{}] LLM delta stream request start provider={}, model={}, tools={}",
                    traceId, stage, providerKey, model, hasTools ? tools.size() : 0);
            callLogger.info(log, "[{}][{}] LLM delta stream system prompt:\n{}", traceId, stage, callLogger.normalizePrompt(systemPrompt));
            callLogger.info(log, "[{}][{}] LLM delta stream history messages count={}", traceId, stage, historyMessages == null ? 0 : historyMessages.size());
            callLogger.logHistoryMessages(log, traceId, stage, historyMessages);
            callLogger.info(log, "[{}][{}] LLM delta stream user prompt:\n{}", traceId, stage, callLogger.normalizePrompt(userPrompt));

            if (chatClient == null) {
                return streamContentInternal(providerKey, model, systemPrompt, historyMessages, userPrompt, stage)
                        .map(content -> new LlmStreamDelta(content, null, null));
            }

            Flux<LlmStreamDelta> deltaFlux;
            if (hasTools) {
                AtomicBoolean rawDeltaEmitted = new AtomicBoolean(false);
                deltaFlux = rawSseClient.streamDeltasRawSse(
                                providerKey,
                                model,
                                systemPrompt,
                                historyMessages,
                                userPrompt,
                                tools,
                                parallelToolCalls,
                                traceId,
                                stage
                        )
                        .doOnNext(ignored -> rawDeltaEmitted.set(true))
                        .onErrorResume(ex -> {
                            if (rawDeltaEmitted.get()) {
                                log.warn(
                                        "[{}][{}] raw delta stream failed after partial output, skip fallback to avoid duplicate stream events",
                                        traceId,
                                        stage,
                                        ex
                                );
                                return Flux.error(ex);
                            }
                            log.warn("[{}][{}] raw delta stream failed, fallback to ChatClient stream", traceId, stage, ex);
                            return streamDeltasByChatClient(chatClient, model, systemPrompt, historyMessages, userPrompt, tools, parallelToolCalls);
                        });
            } else {
                deltaFlux = streamDeltasByChatClient(chatClient, model, systemPrompt, historyMessages, userPrompt, tools, false);
            }

            return deltaFlux
                    .filter(delta -> delta != null
                            && ((StringUtils.hasText(delta.content()))
                            || (delta.toolCalls() != null && !delta.toolCalls().isEmpty())
                            || StringUtils.hasText(delta.finishReason())))
                    .doOnNext(delta -> callLogger.appendDeltaLog(responseBuffer, delta, traceId, stage))
                    .doOnComplete(() -> callLogger.info(
                            log,
                            "[{}][{}] LLM delta stream response finished in {} ms:\n{}",
                            traceId,
                            stage,
                            callLogger.elapsedMs(startNanos),
                            responseBuffer
                    ))
                    .doOnError(ex -> log.error(
                            "[{}][{}] LLM delta stream failed in {} ms, partial response:\n{}",
                            traceId,
                            stage,
                            callLogger.elapsedMs(startNanos),
                            responseBuffer,
                            ex
                    ))
                    .doOnCancel(() -> callLogger.info(
                            log,
                            "[{}][{}] LLM delta stream canceled in {} ms, partial response:\n{}",
                            traceId,
                            stage,
                            callLogger.elapsedMs(startNanos),
                            responseBuffer
                    ))
                    .timeout(Duration.ofSeconds(60));
        });
    }

    private Mono<String> completeTextInternal(
            String providerKey,
            String model,
            String systemPrompt,
            String userPrompt,
            String stage
    ) {
        return Mono.fromCallable(() -> {
                    ChatClient chatClient = resolveChatClient(providerKey);
                    if (chatClient == null) {
                        throw new IllegalStateException("No ChatClient registered for provider key: " + providerKey);
                    }
                    String traceId = callLogger.generateTraceId();
                    long startNanos = System.nanoTime();

                    callLogger.info(log, "[{}][{}] LLM call request start provider={}, model={}", traceId, stage, providerKey, model);
                    callLogger.info(log, "[{}][{}] LLM call system prompt:\n{}", traceId, stage, callLogger.normalizePrompt(systemPrompt));
                    callLogger.info(log, "[{}][{}] LLM call user prompt:\n{}", traceId, stage, callLogger.normalizePrompt(userPrompt));

                    OpenAiChatOptions options = OpenAiChatOptions.builder().model(model).build();
                    ChatClient.ChatClientRequestSpec prompt = chatClient.prompt().options(options);
                    if (StringUtils.hasText(systemPrompt)) {
                        prompt = prompt.system(systemPrompt);
                    }

                    String content = prompt.user(userPrompt)
                            .call()
                            .content();
                    String normalized = content == null ? "" : content;

                    callLogger.info(
                            log,
                            "[{}][{}] LLM call response finished in {} ms:\n{}",
                            traceId,
                            stage,
                            callLogger.elapsedMs(startNanos),
                            callLogger.sanitizeText(normalized)
                    );
                    return normalized;
                })
                .doOnError(ex -> log.error("LLM call failed provider={}, stage={}", providerKey, stage, ex))
                .subscribeOn(Schedulers.boundedElastic())
                .onErrorReturn("");
    }

    private Flux<LlmStreamDelta> streamDeltasByChatClient(
            ChatClient chatClient,
            String model,
            String systemPrompt,
            List<Message> historyMessages,
            String userPrompt,
            List<LlmFunctionTool> tools,
            boolean parallelToolCalls
    ) {
        OpenAiChatOptions options = buildStreamOptions(model, tools, parallelToolCalls);
        ChatClient.ChatClientRequestSpec prompt = chatClient.prompt().options(options);
        if (StringUtils.hasText(systemPrompt)) {
            prompt = prompt.system(systemPrompt);
        }
        if (historyMessages != null && !historyMessages.isEmpty()) {
            prompt = prompt.messages(historyMessages);
        }
        if (StringUtils.hasText(userPrompt)) {
            prompt = prompt.user(userPrompt);
        }

        return prompt.stream()
                .chatResponse()
                .map(this::toStreamDelta);
    }

    private OpenAiChatOptions buildStreamOptions(String model, List<LlmFunctionTool> tools, boolean parallelToolCalls) {
        OpenAiChatOptions.Builder builder = OpenAiChatOptions.builder()
                .model(model)
                // We consume native tool_calls ourselves. Disable Spring AI internal execution.
                .internalToolExecutionEnabled(false);
        if (tools != null && !tools.isEmpty()) {
            List<OpenAiApi.FunctionTool> functionTools = tools.stream()
                    .filter(tool -> tool != null && StringUtils.hasText(tool.name()))
                    .map(this::toOpenAiFunctionTool)
                    .toList();
            if (!functionTools.isEmpty()) {
                builder.tools(functionTools);
                builder.toolChoice("auto");
                builder.parallelToolCalls(parallelToolCalls);
            }
        }
        return builder.build();
    }

    private OpenAiApi.FunctionTool toOpenAiFunctionTool(LlmFunctionTool tool) {
        Map<String, Object> parameters = tool.parameters() == null ? Map.of(
                "type", "object",
                "properties", Map.of(),
                "additionalProperties", true
        ) : tool.parameters();
        OpenAiApi.FunctionTool.Function function = new OpenAiApi.FunctionTool.Function(
                tool.description(),
                tool.name(),
                parameters,
                tool.strict()
        );
        return new OpenAiApi.FunctionTool(function);
    }

    private LlmStreamDelta toStreamDelta(ChatResponse response) {
        if (response == null || response.getResult() == null || response.getResult().getOutput() == null) {
            return new LlmStreamDelta(null, null, null);
        }

        AssistantMessage output = response.getResult().getOutput();
        String content = output.getText();
        List<SseChunk.ToolCall> toolCalls = output.getToolCalls() == null ? List.of()
                : output.getToolCalls().stream()
                .map(call -> new SseChunk.ToolCall(
                        call.id(),
                        call.type(),
                        new SseChunk.Function(call.name(), call.arguments())
                ))
                .toList();
        String finishReason = response.getResult().getMetadata() == null
                ? null
                : response.getResult().getMetadata().getFinishReason();
        return new LlmStreamDelta(content, toolCalls.isEmpty() ? null : toolCalls, finishReason);
    }

    private ProviderType toProviderType(String providerKey) {
        if (!StringUtils.hasText(providerKey)) {
            return null;
        }
        try {
            return ProviderType.valueOf(providerKey.trim().toUpperCase(Locale.ROOT));
        } catch (IllegalArgumentException ignored) {
            return null;
        }
    }

    private String resolveProviderKey(ProviderType providerType) {
        if (providerType == null) {
            return "bailian";
        }
        return providerType.name().toLowerCase(Locale.ROOT);
    }

    private ChatClient resolveChatClient(String providerKey) {
        if (chatClientRegistry != null) {
            ChatClient resolved = chatClientRegistry.getClient(providerKey);
            if (resolved != null) {
                return resolved;
            }
        }
        if (legacyClients.isEmpty()) {
            return null;
        }
        String normalized = StringUtils.hasText(providerKey)
                ? providerKey.trim().toLowerCase(Locale.ROOT)
                : "bailian";
        ChatClient legacy = legacyClients.get(normalized);
        if (legacy != null) {
            return legacy;
        }
        ProviderType providerType = toProviderType(normalized);
        if (providerType == null) {
            return legacyClients.get("bailian");
        }
        return legacyClients.get(providerType.name().toLowerCase(Locale.ROOT));
    }

    private static Map<String, ChatClient> legacyClientMap(ChatClient bailianChatClient, ChatClient siliconflowChatClient) {
        Map<String, ChatClient> clients = new LinkedHashMap<>();
        if (bailianChatClient != null) {
            clients.put("bailian", bailianChatClient);
        }
        if (siliconflowChatClient != null) {
            clients.put("siliconflow", siliconflowChatClient);
        }
        return Map.copyOf(clients);
    }
}
